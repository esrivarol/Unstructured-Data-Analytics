{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ester Rivarola - \n",
    "Assignment 5 \n",
    "\n",
    "##### 3/5/2018\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install all needed packages\n",
    "#Codes used in class and in previous assignment as well\n",
    "\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import collections\n",
    "import nltk\n",
    "import random\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.classify.util import accuracy\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.classify import DecisionTreeClassifier\n",
    "\n",
    "characters_to_remove = [\"''\",'``',\"rt\",\"https\",\"’\",\"“\",\"”\",\"\\u200b\",\"--\",\"n't\",\"'s\",\"...\",\"//t.c\"]\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def bag_of_words(words):\n",
    "    words = word_tokenize(words)\n",
    "    words = [word.lower() for word in words]\n",
    "    words = [word for word in words if word not in set(characters_to_remove)]\n",
    "    words = [word for word in words if word not in stopwords.words(\"english\")]\n",
    "    words = [word for word in words if word not in set(string.punctuation[1:])]\n",
    "\n",
    "    lemma_list = [wordnet_lemmatizer.lemmatize(word) for word in words]\n",
    "    my_dict = dict([(word, True) for word in lemma_list])\n",
    "    return my_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hotel 1: \n",
    "\n",
    "\n",
    "#### Doubletree By Hilton \n",
    "\n",
    "341 West 36th Street, New York City, NY 10018-640\n",
    "    \n",
    "Rating: 3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code borrowed from last assignment \n",
    "\n",
    "browser = webdriver.Firefox()\n",
    "\n",
    "dataset = []\n",
    "\n",
    "for i in range(0,300,5):\n",
    "    url = 'https://www.tripadvisor.com/Hotel_Review-g60763-d1149404-Reviews-or' + str(i)\n",
    "    browser.get(url)\n",
    "    time.sleep(5)\n",
    "    \n",
    "    element_list = browser.find_elements_by_xpath(\"//span[@class='taLnk ulBlueLinks']\")\n",
    "        \n",
    "    for e in element_list:\n",
    "        try:\n",
    "            e.click()\n",
    "        except:\n",
    "            pass\n",
    "    html = browser.page_source\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    containers = soup.find_all('div','reviewSelector')\n",
    "        \n",
    "    for container in containers:\n",
    "        int_rating = int(container.find('span','ui_bubble_rating')['class'][1].split('_')[1])//10\n",
    "        if int_rating>3:\n",
    "            rating = 'pos'\n",
    "        else:\n",
    "            rating = 'neg'\n",
    "        \n",
    "        review = container.p.text\n",
    "        \n",
    "        dataset.append((rating,review))\n",
    "\n",
    "browser.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive: 119\n",
      "Negative: 181\n"
     ]
    }
   ],
   "source": [
    "#to check for distribution of reviews:\n",
    "\n",
    "print('Positive: ' + str(sum(1 for t in dataset if t[0] == 'pos')))\n",
    "print('Negative: ' + str(sum(1 for t in dataset if t[0] == 'neg')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#to find the len of the dataset\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_1 =[(bag_of_words(dataset[i][1]), dataset[i][0]) for i in range(len(dataset))]\n",
    "\n",
    "random.shuffle(dataset_1)\n",
    "\n",
    "train_set, test_set = dataset_1[:int(len(dataset_1)*.75)], dataset_1[int(len(dataset_1)*.75):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.metrics.scores import (accuracy, precision, recall, f_measure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Perfomance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.75\n",
      "pos precision: 0.7\n",
      "pos recall: 0.8\n",
      "pos F-measure: 0.75\n",
      "neg precision: 0.8\n",
      "neg recall: 0.7\n",
      "neg F-measure: 0.75\n"
     ]
    }
   ],
   "source": [
    "tr_sets = collections.defaultdict(set)\n",
    "testsets = collections.defaultdict(set)\n",
    "\n",
    "for i, (feats, label) in enumerate(test_set):\n",
    "    tr_sets[label].add(i)\n",
    "    observed = nb_classifier.classify(feats)\n",
    "    testsets[observed].add(i)\n",
    "    \n",
    "\n",
    "print('Accuracy:', float(round(nltk.classify.accuracy(nb_classifier, test_set),2)))\n",
    "print('pos precision:', float(round(precision(tr_sets['pos'], testsets['pos']),2)))\n",
    "print('pos recall:', float(round(recall(tr_sets['pos'], testsets['pos']),2)))\n",
    "print('pos F-measure:', float(round(f_measure(tr_sets['pos'], testsets['pos']),2)))\n",
    "print('neg precision:', float(round(precision(tr_sets['neg'], testsets['neg']),2)))\n",
    "print('neg recall:', float(round(recall(tr_sets['neg'], testsets['neg']),2)))\n",
    "print('neg F-measure:', float(round(f_measure(tr_sets['neg'], testsets['neg']),2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tress Perfomance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_classifier = DecisionTreeClassifier.train(train_set,binary=True,entropy_cutoff=.8,depth_cutoff=5,support_cutoff=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.69\n",
      "pos precision: 0.88\n",
      "pos recall: 0.4\n",
      "pos F-measure: 0.55\n",
      "neg precision: 0.64\n",
      "neg recall: 0.95\n",
      "neg F-measure: 0.77\n"
     ]
    }
   ],
   "source": [
    "tr_sets1 = collections.defaultdict(set)\n",
    "testsets1 = collections.defaultdict(set)\n",
    "\n",
    "for i, (feats, label) in enumerate(test_set):\n",
    "    tr_sets1[label].add(i)\n",
    "    observed = dt_classifier.classify(feats)\n",
    "    testsets1[observed].add(i)\n",
    "    \n",
    "## Decision Tree Perfomance Metrics    \n",
    "print('Accuracy:', float(round(nltk.classify.accuracy(dt_classifier, test_set),2)))\n",
    "print('pos precision:', float(round(precision(tr_sets1['pos'], testsets1['pos']),2)))\n",
    "print('pos recall:', float(round(recall(tr_sets1['pos'], testsets1['pos']),2)))\n",
    "print('pos F-measure:', float(round(f_measure(tr_sets1['pos'], testsets1['pos']),2)))\n",
    "print('neg precision:', float(round(precision(tr_sets1['neg'], testsets1['neg']),2)))\n",
    "print('neg recall:', float(round(recall(tr_sets1['neg'], testsets1['neg']),2)))\n",
    "print('neg F-measure:', float(round(f_measure(tr_sets1['neg'], testsets1['neg']),2)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this assignment, the metric that would provide better insight is Negative Recall. Decision Trees provided the Highest Negegative Recall of 95%, this indicates that only 5% of the reviews were incorrecly classified as false negatives, and therefore I am choosing Decision Tress as my best performing model and now will try to transfer this knowledge to the second hotel. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HOTEL 2: \n",
    "\n",
    "#### Amsterdam Court Hotel\n",
    "\n",
    "226 West 50th Street, New York City, NY 10019-6702\n",
    "\n",
    "Rating: 3.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser = webdriver.Firefox()\n",
    "dataset2 = []\n",
    "\n",
    "for i in range(0,300,5):\n",
    "    url2 = 'https://www.tripadvisor.com/Hotel_Review-g60763-d80075-Reviews-or' + str(i)\n",
    "    browser.get(url2)\n",
    "    time.sleep(5)\n",
    "    \n",
    "    element_list = browser.find_elements_by_xpath(\"//span[@class='taLnk ulBlueLinks']\")\n",
    "        \n",
    "    for e in element_list:\n",
    "        try:\n",
    "            e.click()\n",
    "        except:\n",
    "            pass\n",
    "    html = browser.page_source\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    containers = soup.find_all('div','reviewSelector')\n",
    "        \n",
    "    for container in containers:\n",
    "        int_rating = int(container.find('span','ui_bubble_rating')['class'][1].split('_')[1])//10\n",
    "        if int_rating>3:\n",
    "            rating = 'pos'\n",
    "        else:\n",
    "            rating = 'neg'\n",
    "        \n",
    "        review = container.p.text\n",
    "        \n",
    "        dataset2.append((rating,review))\n",
    "\n",
    "browser.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive: 185\n",
      "Negative: 115\n"
     ]
    }
   ],
   "source": [
    "#to check for distribution of reviews:\n",
    "\n",
    "print('Positive: ' + str(sum(1 for t in dataset2 if t[0] == 'pos')))\n",
    "print('Negative: ' + str(sum(1 for t in dataset2 if t[0] == 'neg')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for_dataset2 =[(bag_of_words(dataset2[i][1]), dataset2[i][0]) for i in range(len(dataset2))]\n",
    "\n",
    "random.shuffle(for_dataset2)\n",
    "\n",
    "testset2 = for_dataset2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.54\n",
      "pos precision: 0.76\n",
      "pos recall: 0.38\n",
      "pos F-measure: 0.51\n",
      "neg precision: 0.45\n",
      "neg recall: 0.81\n",
      "neg F-measure: 0.58\n"
     ]
    }
   ],
   "source": [
    "tr_sets3 = collections.defaultdict(set)\n",
    "t_sets3 = collections.defaultdict(set)\n",
    "\n",
    "for i, (feats, label) in enumerate(testset2):\n",
    "    tr_sets3[label].add(i)\n",
    "    observed1 = dt_classifier.classify(feats)\n",
    "    t_sets3[observed1].add(i)\n",
    "    \n",
    "\n",
    "## Decision Tree Perfomance Metrics    \n",
    "print('Accuracy:', float(round(nltk.classify.accuracy(dt_classifier, testset2),2)))\n",
    "print('pos precision:', float(round(precision(tr_sets3['pos'], t_sets3['pos']),2)))\n",
    "print('pos recall:', float(round(recall(tr_sets3['pos'], t_sets3['pos']),2)))\n",
    "print('pos F-measure:', float(round(f_measure(tr_sets3['pos'], t_sets3['pos']),2)))\n",
    "print('neg precision:', float(round(precision(tr_sets3['neg'], t_sets3['neg']),2)))\n",
    "print('neg recall:', float(round(recall(tr_sets3['neg'], t_sets3['neg']),2)))\n",
    "print('neg F-measure:', float(round(f_measure(tr_sets3['neg'], t_sets3['neg']),2)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performace of each metrics had dropped but the negative recall is still signigicantly high 81% -- I would stop it here, but I want to compare this results with the the Naive Bayes Model too, to see if the metrics increases or decreases..let's see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.73\n",
      "pos precision: 0.72\n",
      "pos recall: 0.91\n",
      "pos F-measure: 0.8\n",
      "neg precision: 0.75\n",
      "neg recall: 0.43\n",
      "neg F-measure: 0.55\n"
     ]
    }
   ],
   "source": [
    "tr_sets4 = collections.defaultdict(set)\n",
    "t_sets4 = collections.defaultdict(set)\n",
    "\n",
    "for i, (feats, label) in enumerate(testset2):\n",
    "    tr_sets4[label].add(i)\n",
    "    observed1 = nb_classifier.classify(feats)\n",
    "    t_sets4[observed1].add(i)\n",
    "    \n",
    "    \n",
    "## Naive Bayes Perfomance Metrics    \n",
    "print('Accuracy:', float(round(nltk.classify.accuracy(nb_classifier, testset2),2)))\n",
    "print('pos precision:', float(round(precision(tr_sets4['pos'], t_sets4['pos']),2)))\n",
    "print('pos recall:', float(round(recall(tr_sets4['pos'], t_sets4['pos']),2)))\n",
    "print('pos F-measure:', float(round(f_measure(tr_sets4['pos'], t_sets4['pos']),2)))\n",
    "print('neg precision:', float(round(precision(tr_sets4['neg'], t_sets4['neg']),2)))\n",
    "print('neg recall:', float(round(recall(tr_sets4['neg'], t_sets4['neg']),2)))\n",
    "print('neg F-measure:', float(round(f_measure(tr_sets4['neg'], t_sets4['neg']),2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well..the level of accuracy increased, but the level of neg recall decreased dramatically. Therefore, I can conclude that the best model for this specific assignment is Decison Tree."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
